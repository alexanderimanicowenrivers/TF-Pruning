{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST-Prune.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexanderimanicowenrivers/TF-Pruning/blob/master/MNIST_Prune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "onXwj-Mxblkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Intro "
      ]
    },
    {
      "metadata": {
        "id": "HZ9PK-6XZ36u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this colab document we will explore how to create and prune a basic model in tensorflow.\n"
      ]
    },
    {
      "metadata": {
        "id": "qn3nWaMHboup",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports & utils"
      ]
    },
    {
      "metadata": {
        "id": "7_JNrbEeYeuz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow.contrib.eager as tfe\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UKYEpps-iGRO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot learning curves of experiments\n",
        "def plot_learning_curves(experiment_data):\n",
        "  # Generate figure.\n",
        "  fig, axes = plt.subplots(3, 4, figsize=(22,12))\n",
        "  st = fig.suptitle(\n",
        "      \"Learning Curves for all Tasks and Hyper-parameter settings\",\n",
        "      fontsize=\"x-large\")\n",
        "  # Plot all learning curves.\n",
        "  for i, results in enumerate(experiment_data):\n",
        "    for j, (setting, train_accuracy, test_accuracy) in enumerate(results):\n",
        "      # Plot.\n",
        "      xs = [x * log_period_samples for x in range(1, len(train_accuracy)+1)]\n",
        "      axes[j, i].plot(xs, train_accuracy, label='train_accuracy')\n",
        "      axes[j, i].plot(xs, test_accuracy, label='test_accuracy')\n",
        "      # Prettify individual plots.\n",
        "      axes[j, i].ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
        "      axes[j, i].set_xlabel('Number of samples processed')\n",
        "      axes[j, i].set_ylabel('Epochs: {}, Learning rate: {}.  Accuracy'.format(*setting))\n",
        "      axes[j, i].set_title('Task {}'.format(i + 1))\n",
        "      axes[j, i].legend()\n",
        "  # Prettify overall figure.\n",
        "  plt.tight_layout()\n",
        "  st.set_y(0.95)\n",
        "  fig.subplots_adjust(top=0.91)\n",
        "  plt.show()\n",
        "\n",
        "# Generate summary table of results.\n",
        "def plot_summary_table(experiment_data):\n",
        "  # Fill Data.\n",
        "  cell_text = []\n",
        "  rows = []\n",
        "  columns = ['Setting 1', 'Setting 2', 'Setting 3']\n",
        "  for i, results in enumerate(experiment_data):\n",
        "    rows.append('Model {}'.format(i + 1))\n",
        "    cell_text.append([])\n",
        "    for j, (setting, train_accuracy, test_accuracy) in enumerate(results):\n",
        "      cell_text[i].append(test_accuracy[-1])\n",
        "  # Generate Table.\n",
        "  fig=plt.figure(frameon=False)\n",
        "  ax = plt.gca()\n",
        "  the_table = ax.table(\n",
        "      cellText=cell_text,\n",
        "      rowLabels=rows,\n",
        "      colLabels=columns,\n",
        "      loc='center')\n",
        "  the_table.scale(1, 4)\n",
        "  # Prettify.\n",
        "  ax.patch.set_facecolor('None')\n",
        "  ax.xaxis.set_visible(False)\n",
        "  ax.yaxis.set_visible(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mw0BAG84Y2bW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data loader"
      ]
    },
    {
      "metadata": {
        "id": "s7czD-PNiRT2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Global variables.\n",
        "log_period_samples = 20000\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ps_IQMZY_Em",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    return input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a3CkxirMY1JX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9342a33f-fe66-4806-efca-ee1522e622d1"
      },
      "cell_type": "code",
      "source": [
        "  mnist=get_data()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CsnxVdc0hag3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Store results of runs with different configurations in a dictionary.\n",
        "# Use a tuple (num_epochs, learning_rate) as keys, and a tuple (training_accuracy, testing_accuracy)\n",
        "experiments_task1 = []\n",
        "settings = [(5, 0.005)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T0C6d-7IZDsJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "kN-D6W57ZEtp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weight_default_name='weight'\n",
        "bias_default_name='bias'\n",
        "\n",
        "def get_placeholders():\n",
        "  x = tf.placeholder(tf.float32, [None, 784])\n",
        "  y_ = tf.placeholder(tf.float32, [None, 10])\n",
        "  return x, y_\n",
        "\n",
        "def _weight_variable(\n",
        "        shape,\n",
        "        initializer=None,\n",
        "        name=None,\n",
        "        layer_no=0,):\n",
        "    \"\"\"\n",
        "    Returns a weight variable with a given shape.\n",
        "    :param initializer: TensorFlow initializer. Default Xavier.\n",
        "    :param layer: Variable layer number.\n",
        "    :param shape: var shape.\n",
        "    \"\"\"\n",
        "    if name==weight_default_name: \n",
        "      name=weight_default_name+'_'+str(layer_no)\n",
        "      \n",
        "    if initializer is None:\n",
        "        initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "    var = tf.get_variable(name, shape, initializer=initializer)\n",
        "    return var\n",
        "  \n",
        "def _bias_variable(\n",
        "        shape,\n",
        "        initializer=None,\n",
        "        layer_no=0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a bias variable with a given shape.\n",
        "    :param initializer: TensorFlow initializer. Default zero.\n",
        "    :param layer_no: Variable layer number.\n",
        "    :param shape: Variable shape.\n",
        "    \"\"\"\n",
        "    name=bias_default_name+'_'+str(layer_no)\n",
        "    if initializer is None:\n",
        "        initializer = tf.constant_initializer(0.)\n",
        "\n",
        "    return _weight_variable(shape,\n",
        "                            initializer=initializer,\n",
        "                            name=name)\n",
        "  \n",
        "  \n",
        "def affine(\n",
        "        inp,\n",
        "        units,\n",
        "        bias=True,\n",
        "        W_initializer=None,\n",
        "        b_initializer=None,\n",
        "        W_name=weight_default_name,\n",
        "        bias_name=bias_default_name,\n",
        "        layer_no=0\n",
        "):\n",
        "    \"\"\" Creates an affine layer.\n",
        "    :param inp: Input tensor.\n",
        "    :param units: Number of units.\n",
        "    :param bias: Include bias term.\n",
        "    :param W_initializer: Initializer for the multiplicative weight.\n",
        "    :param b_initializer: Initializer for the bias term.\n",
        "    :param W_name: Name of the weight.\n",
        "    :param bias_name: Name of the bias.\n",
        "    :return: Tensor defined as input.dot(weight) + bias.\n",
        "    \"\"\"\n",
        "    input_size = inp.shape[-1]\n",
        "    W = _weight_variable([input_size, units],\n",
        "                         initializer=W_initializer,\n",
        "                         name=W_name,layer_no=layer_no)\n",
        "\n",
        "    output = tf.matmul(inp, W)\n",
        "\n",
        "    if bias:\n",
        "        b = _bias_variable((units,),\n",
        "                           initializer=b_initializer,\n",
        "                           layer_no=layer_no)\n",
        "\n",
        "        output=tf.add(output, b)\n",
        "\n",
        "    return output\n",
        "  \n",
        "def mlp(inputs,\n",
        "        layer_sizes,\n",
        "        nonlinearity=tf.nn.relu,\n",
        "        output_nonlinearity=None,\n",
        "        W_initializer=None,\n",
        "        b_initializer=None):\n",
        "    \"\"\"\n",
        "    Creates a multi-layer perceptron with given hidden sizes. A nonlinearity\n",
        "    is applied after every hidden layer.\n",
        "    \n",
        "    output shape: N x (number of output units)\n",
        "    :param inputs: List of input tensors.\n",
        "    :param layer_sizes: List of layers sizes, including output layer size.\n",
        "    :param nonlinearity: Hidden layer nonlinearity.\n",
        "    :param output_nonlinearity: Output layer nonlinearity.\n",
        "    :param W_initializer: Weight initializer.\n",
        "    :param b_initializer: Bias initializer.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if layer_sizes[-1] is None:\n",
        "        layer_sizes = list(layer_sizes)\n",
        "        layer_sizes[-1] = 1\n",
        "\n",
        "    # Take care of the input layer separately to make use of broadcasting in\n",
        "    # a case of several input tensors.\n",
        "    layer = affine(\n",
        "    inp=inputs,\n",
        "    units=layer_sizes[0],\n",
        "    bias=False,\n",
        "    W_initializer=W_initializer,\n",
        "    b_initializer=b_initializer\n",
        "    )\n",
        "\n",
        "    layer = nonlinearity(layer)\n",
        "\n",
        "    for i_layer, size in enumerate(layer_sizes[1:], 1):\n",
        "\n",
        "      layer = affine(layer, size,\n",
        "                     W_initializer=W_initializer,\n",
        "                     b_initializer=b_initializer,\n",
        "                    layer_no=i_layer)\n",
        "      if i_layer < len(layer_sizes) - 1:\n",
        "          layer = nonlinearity(layer)\n",
        "\n",
        "    return layer\n",
        "  \n",
        "def get_graph():\n",
        "  x, y_ = get_placeholders()\n",
        "  linear=mlp(x,[1000, 1000, 500, 200,10])\n",
        "  loss=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=linear,labels=y_))\n",
        "  opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "  correct_prediction = tf.equal(tf.argmax(linear,1), tf.argmax(y_,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  \n",
        "  return x, y_,linear,loss,opt,correct_prediction,accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_q0UztOoZG0k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "WX27U9PcCe18",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "d1ed131f-73ea-4b62-8a51-0a4e10762132"
      },
      "cell_type": "code",
      "source": [
        "print('Training Model 1')\n",
        "\n",
        "# Train Model 1 with the different hyper-parameter settings.\n",
        "for (num_epochs, learning_rate) in settings:\n",
        "\n",
        "  # Reset graph, recreate placeholders and dataset.\n",
        "  tf.reset_default_graph()\n",
        "  mnist = get_data()\n",
        "  eval_mnist = get_data()\n",
        "\n",
        "  #####################################################\n",
        "  # Define model, loss, update and evaluation metric. #\n",
        "  #####################################################\n",
        "  x, y_,linear,loss,opt,correct_prediction,accuracy=get_graph()\n",
        "  # Train.\n",
        "  i, train_accuracy, test_accuracy = 0, [], []\n",
        "  log_period_updates = int(log_period_samples / batch_size)\n",
        "  saver = tf.train.Saver()\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    while mnist.train.epochs_completed < num_epochs:\n",
        "      \n",
        "      # Update.\n",
        "      i += 1\n",
        "      batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "      \n",
        "      #################\n",
        "      # Training step #\n",
        "      #################\n",
        "      feed_dict={x:batch_xs,y_:batch_ys}\n",
        "      _=sess.run([opt],feed_dict=feed_dict)\n",
        "      \n",
        "      # Periodically evaluate.\n",
        "      if i % log_period_updates == 0:\n",
        "        \n",
        "        #####################################\n",
        "        # Compute and store train accuracy. #\n",
        "        #####################################\n",
        "        \n",
        "        batch_xs, batch_ys = mnist.train.next_batch((int(mnist.train.labels.shape[0]/5)))\n",
        "        \n",
        "        feed_dict={x:batch_xs,y_:batch_ys}\n",
        "        acc=sess.run([accuracy],feed_dict)\n",
        "        print(f'Accuracy {acc} at iteration {i}')\n",
        "\n",
        "        train_accuracy.append(acc)\n",
        "        #####################################\n",
        "        # Compute and store test accuracy.  #\n",
        "        #####################################\n",
        "    save_path = saver.save(sess, \"./model.ckpt\")\n",
        "\n",
        "    feed_dict={x:eval_mnist.test.images,y_:eval_mnist.test.labels}\n",
        "    acc=sess.run([accuracy],feed_dict)\n",
        "    test_accuracy.append(acc)\n",
        "  \n",
        "  experiments_task1.append(\n",
        "      ((num_epochs, learning_rate), train_accuracy, test_accuracy))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model 1\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Accuracy [0.9398182] at iteration 200\n",
            "Accuracy [0.9578182] at iteration 400\n",
            "Accuracy [0.9628182] at iteration 600\n",
            "Accuracy [0.96327275] at iteration 800\n",
            "Accuracy [0.95945454] at iteration 1000\n",
            "Accuracy [0.96863633] at iteration 1200\n",
            "Accuracy [0.973] at iteration 1400\n",
            "Accuracy [0.9718182] at iteration 1600\n",
            "Accuracy [0.9728182] at iteration 1800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ww1MmQBfHFw9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f2cb31fb-8dfc-4dc8-a9b7-1a6887e79788"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "x, y_,linear,loss,opt,correct_prediction,accuracy=get_graph()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  # Restore variables from disk.\n",
        "  saver.restore(sess, \"./model.ckpt\")\n",
        "  print(\"Model restored.\")\n",
        "  # Check the values of the variables\n",
        "  all_vars = tf.get_collection('vars')\n",
        "  batch_xs, batch_ys = mnist.train.next_batch((int(mnist.train.labels.shape[0]/5)))\n",
        "  feed_dict={x:batch_xs,y_:batch_ys}\n",
        "  acc=sess.run([accuracy],feed_dict)\n",
        "  print(f'Accuracy {acc} at iteration {i}')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
            "Model restored.\n",
            "Accuracy [0.97409093] at iteration 1800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "51maYRzjZIYi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Â Pruning"
      ]
    },
    {
      "metadata": {
        "id": "b6B8fxaQZJRy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gdxTTrZuZKN8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ]
    },
    {
      "metadata": {
        "id": "LubkBwM9ZLUd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}